{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/zeyuanxu/Desktop/testdir/crawler_v1/code/tweettext.txt\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import gensim\n",
    "import sklearn\n",
    "from sklearn import ensemble,svm,neural_network,discriminant_analysis\n",
    "from sklearn.metrics import roc_curve,auc,precision_recall_curve\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "with open(\"sent_corpus.csv\", \"r\") as sent_file:\n",
    "    lines = sent_file.read().split(\"\\n\")\n",
    "\n",
    "rows = [line.split(\",\") for line in lines if line]\n",
    "rows = [row[:3] + [\",\".join(row[3:])] for row in rows]\n",
    "# remove document start character\n",
    "rows[0][0] = rows[0][0][1:]\n",
    "sentDf_cols = ['ItemID', 'Sentiment', 'SentimentSource', 'SentimentText']\n",
    "sentDf = pd.DataFrame(rows[1:],columns=sentDf_cols)\n",
    "# print(sentDf.columns.values)\n",
    "sentDf[[\"ItemID\",\"Sentiment\"]] = sentDf[[\"ItemID\",\"Sentiment\"]].astype(int)\n",
    "sentDf[\"SentimentText\"] = sentDf[\"SentimentText\"].apply(lambda text: text.split())\n",
    "## -*- coding: utf-8 -*-\n",
    "curdir = os.getcwd()\n",
    "tweet_path = curdir + \"/crawler_v1/code/tweettext.txt\"\n",
    "print(tweet_path)\n",
    "\n",
    "tweets = []\n",
    "tweet_file = open(tweet_path, \"rb\")\n",
    "for line in tweet_file:\n",
    "#    print(line)\n",
    "#    line = line_pre.decode('utf-8')\n",
    "    tweet = \"\"\n",
    "    if(line[:2] == \"RT\"):\n",
    "        char_loc = 0\n",
    "        for char in line:\n",
    "            char_loc += 1\n",
    "            if(char==\":\"):\n",
    "                break\n",
    "\n",
    "        tweet = line[-(len(line)-char_loc):]\n",
    "    else:\n",
    "        tweet = line\n",
    "    tweets.append(tweet)\n",
    "\n",
    "#[tweet.decode('utf-8') for tweet in tweets]\n",
    "# print(tweets)\n",
    "# print(\"done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ItemID</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>SentimentSource</th>\n",
       "      <th>SentimentText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Sentiment140</td>\n",
       "      <td>[is, so, sad, for, my, APL, friend.............]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>Sentiment140</td>\n",
       "      <td>[I, missed, the, New, Moon, trailer...]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>Sentiment140</td>\n",
       "      <td>[omg, its, already, 7:30, :O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>Sentiment140</td>\n",
       "      <td>[.., Omgaga., Im, sooo, im, gunna, CRy., I've,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>Sentiment140</td>\n",
       "      <td>[i, think, mi, bf, is, cheating, on, me!!!, T_T]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>Sentiment140</td>\n",
       "      <td>[or, i, just, worry, too, much?]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>Sentiment140</td>\n",
       "      <td>[Juuuuuuuuuuuuuuuuussssst, Chillin!!]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>Sentiment140</td>\n",
       "      <td>[Sunny, Again, Work, Tomorrow, :-|, TV, Tonight]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>Sentiment140</td>\n",
       "      <td>[handed, in, my, uniform, today, ., i, miss, y...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>Sentiment140</td>\n",
       "      <td>[hmmmm...., i, wonder, how, she, my, number, @-)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>Sentiment140</td>\n",
       "      <td>[I, must, think, about, positive..]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>Sentiment140</td>\n",
       "      <td>[thanks, to, all, the, haters, up, in, my, fac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>Sentiment140</td>\n",
       "      <td>[this, weekend, has, sucked, so, far]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>Sentiment140</td>\n",
       "      <td>[jb, isnt, showing, in, australia, any, more!]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>Sentiment140</td>\n",
       "      <td>[ok, thats, it, you, win.]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>Sentiment140</td>\n",
       "      <td>[&amp;lt;--------, This, is, the, way, i, feel, ri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>Sentiment140</td>\n",
       "      <td>[\", awhhe, man...., I'm, completely, useless, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td>Sentiment140</td>\n",
       "      <td>[Feeling, strangely, fine., Now, I'm, gonna, g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>Sentiment140</td>\n",
       "      <td>[HUGE, roll, of, thunder, just, now...SO, scar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>Sentiment140</td>\n",
       "      <td>[I, just, cut, my, beard, off., It's, only, be...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>Sentiment140</td>\n",
       "      <td>[Very, sad, about, Iran.]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>Sentiment140</td>\n",
       "      <td>[wompppp, wompp]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>23</td>\n",
       "      <td>1</td>\n",
       "      <td>Sentiment140</td>\n",
       "      <td>[You're, the, only, one, who, can, see, this, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>Sentiment140</td>\n",
       "      <td>[&amp;lt;---Sad, level, is, 3., I, was, writing, a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "      <td>Sentiment140</td>\n",
       "      <td>[..., Headed, to, Hospitol, :, Had, to, pull, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>26</td>\n",
       "      <td>0</td>\n",
       "      <td>Sentiment140</td>\n",
       "      <td>[BoRinG, ):, whats, wrong, with, him??, Please...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "      <td>Sentiment140</td>\n",
       "      <td>[can't, be, bothered., i, wish, i, could, spen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>28</td>\n",
       "      <td>0</td>\n",
       "      <td>Sentiment140</td>\n",
       "      <td>[\", Feeeling, like, shit, right, now., I, real...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>29</td>\n",
       "      <td>1</td>\n",
       "      <td>Sentiment140</td>\n",
       "      <td>[\", goodbye, exams,, HELLO, ALCOHOL, TONIGHT, \"]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>Sentiment140</td>\n",
       "      <td>[I, didn't, realize, it, was, THAT, deep., Gee...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1578597</th>\n",
       "      <td>1578598</td>\n",
       "      <td>1</td>\n",
       "      <td>Sentiment140</td>\n",
       "      <td>[zoo, was, rad, today., feeling, tired, and, n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1578598</th>\n",
       "      <td>1578599</td>\n",
       "      <td>1</td>\n",
       "      <td>Sentiment140</td>\n",
       "      <td>[Zoo, with, the, woman]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1578599</th>\n",
       "      <td>1578600</td>\n",
       "      <td>0</td>\n",
       "      <td>Sentiment140</td>\n",
       "      <td>[zoolander, and, alice, in, wonderland., i, ha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1578600</th>\n",
       "      <td>1578601</td>\n",
       "      <td>1</td>\n",
       "      <td>Sentiment140</td>\n",
       "      <td>[Zoom, zoom!, Back, to, bristol, today, I, hav...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1578601</th>\n",
       "      <td>1578602</td>\n",
       "      <td>0</td>\n",
       "      <td>Sentiment140</td>\n",
       "      <td>[zootm:, cannot, survive, without, CRLF, suppo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1578602</th>\n",
       "      <td>1578603</td>\n",
       "      <td>0</td>\n",
       "      <td>Sentiment140</td>\n",
       "      <td>[Zoran, lost, Croatian, Idol!, The, difference...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1578603</th>\n",
       "      <td>1578604</td>\n",
       "      <td>0</td>\n",
       "      <td>Sentiment140</td>\n",
       "      <td>[Zork., Buggy, beta, version]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1578604</th>\n",
       "      <td>1578605</td>\n",
       "      <td>1</td>\n",
       "      <td>Sentiment140</td>\n",
       "      <td>[\"Zow,, finished, uploading, pictures, on, Fli...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1578605</th>\n",
       "      <td>1578606</td>\n",
       "      <td>1</td>\n",
       "      <td>Sentiment140</td>\n",
       "      <td>[Zrock, was, awesome!!]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1578606</th>\n",
       "      <td>1578607</td>\n",
       "      <td>1</td>\n",
       "      <td>Sentiment140</td>\n",
       "      <td>[ZTecWiz, bought, mIRC, for, $10]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1578607</th>\n",
       "      <td>1578608</td>\n",
       "      <td>1</td>\n",
       "      <td>Sentiment140</td>\n",
       "      <td>['Zu, SpÃ¤t', by, Die, Ã„rzte., One, of, the, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1578608</th>\n",
       "      <td>1578609</td>\n",
       "      <td>1</td>\n",
       "      <td>Sentiment140</td>\n",
       "      <td>[Zuma, bitch, tomorrow., Have, a, wonderful, n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1578609</th>\n",
       "      <td>1578610</td>\n",
       "      <td>0</td>\n",
       "      <td>Sentiment140</td>\n",
       "      <td>[zummie's, couch, tour, was, amazing....to, ba...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1578610</th>\n",
       "      <td>1578611</td>\n",
       "      <td>0</td>\n",
       "      <td>Sentiment140</td>\n",
       "      <td>[\"ZuneHD, looks, great!, OLED, screen, @720p,,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1578611</th>\n",
       "      <td>1578612</td>\n",
       "      <td>1</td>\n",
       "      <td>Sentiment140</td>\n",
       "      <td>[zup, there, !, learning, a, new, magic, trick]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1578612</th>\n",
       "      <td>1578613</td>\n",
       "      <td>1</td>\n",
       "      <td>Sentiment140</td>\n",
       "      <td>[zyklonic, showers, *evil*]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1578613</th>\n",
       "      <td>1578614</td>\n",
       "      <td>1</td>\n",
       "      <td>Sentiment140</td>\n",
       "      <td>[ZZ, Top, â€“, I, Thank, You, ...@hawaiibuzz, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1578614</th>\n",
       "      <td>1578615</td>\n",
       "      <td>0</td>\n",
       "      <td>Sentiment140</td>\n",
       "      <td>[zzz, time., Just, wish, my, love, could, B, n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1578615</th>\n",
       "      <td>1578616</td>\n",
       "      <td>1</td>\n",
       "      <td>Sentiment140</td>\n",
       "      <td>[zzz, twitter., good, day, today., got, a, lot...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1578616</th>\n",
       "      <td>1578617</td>\n",
       "      <td>1</td>\n",
       "      <td>Sentiment140</td>\n",
       "      <td>[\"zzz's, time,, goodnight., http://plurk.com/p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1578617</th>\n",
       "      <td>1578618</td>\n",
       "      <td>0</td>\n",
       "      <td>Sentiment140</td>\n",
       "      <td>[Zzzz, lying, in, bed, watching, the, countrys...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1578618</th>\n",
       "      <td>1578619</td>\n",
       "      <td>1</td>\n",
       "      <td>Sentiment140</td>\n",
       "      <td>[Zzzz..., Fuck, Ã¼, :, Zzzz..., Fuck, Ã¼, http...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1578619</th>\n",
       "      <td>1578620</td>\n",
       "      <td>1</td>\n",
       "      <td>Sentiment140</td>\n",
       "      <td>[Zzzz...no, work, tomorrow..yayyy!!]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1578620</th>\n",
       "      <td>1578621</td>\n",
       "      <td>1</td>\n",
       "      <td>Sentiment140</td>\n",
       "      <td>[ZZZZZ, time.., Tomorrow, will, be, a, busy, d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1578621</th>\n",
       "      <td>1578622</td>\n",
       "      <td>0</td>\n",
       "      <td>Sentiment140</td>\n",
       "      <td>[Zzzzz, want, to, sleep, but, at, sister's, in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1578622</th>\n",
       "      <td>1578623</td>\n",
       "      <td>1</td>\n",
       "      <td>Sentiment140</td>\n",
       "      <td>[Zzzzzz...., Finally!, Night, tweeters!]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1578623</th>\n",
       "      <td>1578624</td>\n",
       "      <td>1</td>\n",
       "      <td>Sentiment140</td>\n",
       "      <td>[\"Zzzzzzz,, sleep, well, people, \"]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1578624</th>\n",
       "      <td>1578625</td>\n",
       "      <td>0</td>\n",
       "      <td>Sentiment140</td>\n",
       "      <td>[ZzzZzZzzzZ..., wait, no, I, have, homework.]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1578625</th>\n",
       "      <td>1578626</td>\n",
       "      <td>0</td>\n",
       "      <td>Sentiment140</td>\n",
       "      <td>[\"ZzZzzzZZZZzzz, meh,, what, am, I, doing, up,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1578626</th>\n",
       "      <td>1578627</td>\n",
       "      <td>0</td>\n",
       "      <td>Sentiment140</td>\n",
       "      <td>[\"Zzzzzzzzzzzzzzzzzzz,, I, wish, \"]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1578627 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          ItemID  Sentiment SentimentSource  \\\n",
       "0              1          0    Sentiment140   \n",
       "1              2          0    Sentiment140   \n",
       "2              3          1    Sentiment140   \n",
       "3              4          0    Sentiment140   \n",
       "4              5          0    Sentiment140   \n",
       "5              6          0    Sentiment140   \n",
       "6              7          1    Sentiment140   \n",
       "7              8          0    Sentiment140   \n",
       "8              9          1    Sentiment140   \n",
       "9             10          1    Sentiment140   \n",
       "10            11          0    Sentiment140   \n",
       "11            12          1    Sentiment140   \n",
       "12            13          0    Sentiment140   \n",
       "13            14          0    Sentiment140   \n",
       "14            15          0    Sentiment140   \n",
       "15            16          0    Sentiment140   \n",
       "16            17          0    Sentiment140   \n",
       "17            18          1    Sentiment140   \n",
       "18            19          0    Sentiment140   \n",
       "19            20          0    Sentiment140   \n",
       "20            21          0    Sentiment140   \n",
       "21            22          0    Sentiment140   \n",
       "22            23          1    Sentiment140   \n",
       "23            24          0    Sentiment140   \n",
       "24            25          0    Sentiment140   \n",
       "25            26          0    Sentiment140   \n",
       "26            27          0    Sentiment140   \n",
       "27            28          0    Sentiment140   \n",
       "28            29          1    Sentiment140   \n",
       "29            30          0    Sentiment140   \n",
       "...          ...        ...             ...   \n",
       "1578597  1578598          1    Sentiment140   \n",
       "1578598  1578599          1    Sentiment140   \n",
       "1578599  1578600          0    Sentiment140   \n",
       "1578600  1578601          1    Sentiment140   \n",
       "1578601  1578602          0    Sentiment140   \n",
       "1578602  1578603          0    Sentiment140   \n",
       "1578603  1578604          0    Sentiment140   \n",
       "1578604  1578605          1    Sentiment140   \n",
       "1578605  1578606          1    Sentiment140   \n",
       "1578606  1578607          1    Sentiment140   \n",
       "1578607  1578608          1    Sentiment140   \n",
       "1578608  1578609          1    Sentiment140   \n",
       "1578609  1578610          0    Sentiment140   \n",
       "1578610  1578611          0    Sentiment140   \n",
       "1578611  1578612          1    Sentiment140   \n",
       "1578612  1578613          1    Sentiment140   \n",
       "1578613  1578614          1    Sentiment140   \n",
       "1578614  1578615          0    Sentiment140   \n",
       "1578615  1578616          1    Sentiment140   \n",
       "1578616  1578617          1    Sentiment140   \n",
       "1578617  1578618          0    Sentiment140   \n",
       "1578618  1578619          1    Sentiment140   \n",
       "1578619  1578620          1    Sentiment140   \n",
       "1578620  1578621          1    Sentiment140   \n",
       "1578621  1578622          0    Sentiment140   \n",
       "1578622  1578623          1    Sentiment140   \n",
       "1578623  1578624          1    Sentiment140   \n",
       "1578624  1578625          0    Sentiment140   \n",
       "1578625  1578626          0    Sentiment140   \n",
       "1578626  1578627          0    Sentiment140   \n",
       "\n",
       "                                             SentimentText  \n",
       "0         [is, so, sad, for, my, APL, friend.............]  \n",
       "1                  [I, missed, the, New, Moon, trailer...]  \n",
       "2                            [omg, its, already, 7:30, :O]  \n",
       "3        [.., Omgaga., Im, sooo, im, gunna, CRy., I've,...  \n",
       "4         [i, think, mi, bf, is, cheating, on, me!!!, T_T]  \n",
       "5                         [or, i, just, worry, too, much?]  \n",
       "6                    [Juuuuuuuuuuuuuuuuussssst, Chillin!!]  \n",
       "7         [Sunny, Again, Work, Tomorrow, :-|, TV, Tonight]  \n",
       "8        [handed, in, my, uniform, today, ., i, miss, y...  \n",
       "9        [hmmmm...., i, wonder, how, she, my, number, @-)]  \n",
       "10                     [I, must, think, about, positive..]  \n",
       "11       [thanks, to, all, the, haters, up, in, my, fac...  \n",
       "12                   [this, weekend, has, sucked, so, far]  \n",
       "13          [jb, isnt, showing, in, australia, any, more!]  \n",
       "14                              [ok, thats, it, you, win.]  \n",
       "15       [&lt;--------, This, is, the, way, i, feel, ri...  \n",
       "16       [\", awhhe, man...., I'm, completely, useless, ...  \n",
       "17       [Feeling, strangely, fine., Now, I'm, gonna, g...  \n",
       "18       [HUGE, roll, of, thunder, just, now...SO, scar...  \n",
       "19       [I, just, cut, my, beard, off., It's, only, be...  \n",
       "20                               [Very, sad, about, Iran.]  \n",
       "21                                        [wompppp, wompp]  \n",
       "22       [You're, the, only, one, who, can, see, this, ...  \n",
       "23       [&lt;---Sad, level, is, 3., I, was, writing, a...  \n",
       "24       [..., Headed, to, Hospitol, :, Had, to, pull, ...  \n",
       "25       [BoRinG, ):, whats, wrong, with, him??, Please...  \n",
       "26       [can't, be, bothered., i, wish, i, could, spen...  \n",
       "27       [\", Feeeling, like, shit, right, now., I, real...  \n",
       "28        [\", goodbye, exams,, HELLO, ALCOHOL, TONIGHT, \"]  \n",
       "29       [I, didn't, realize, it, was, THAT, deep., Gee...  \n",
       "...                                                    ...  \n",
       "1578597  [zoo, was, rad, today., feeling, tired, and, n...  \n",
       "1578598                            [Zoo, with, the, woman]  \n",
       "1578599  [zoolander, and, alice, in, wonderland., i, ha...  \n",
       "1578600  [Zoom, zoom!, Back, to, bristol, today, I, hav...  \n",
       "1578601  [zootm:, cannot, survive, without, CRLF, suppo...  \n",
       "1578602  [Zoran, lost, Croatian, Idol!, The, difference...  \n",
       "1578603                      [Zork., Buggy, beta, version]  \n",
       "1578604  [\"Zow,, finished, uploading, pictures, on, Fli...  \n",
       "1578605                            [Zrock, was, awesome!!]  \n",
       "1578606                  [ZTecWiz, bought, mIRC, for, $10]  \n",
       "1578607  ['Zu, SpÃ¤t', by, Die, Ã„rzte., One, of, the, ...  \n",
       "1578608  [Zuma, bitch, tomorrow., Have, a, wonderful, n...  \n",
       "1578609  [zummie's, couch, tour, was, amazing....to, ba...  \n",
       "1578610  [\"ZuneHD, looks, great!, OLED, screen, @720p,,...  \n",
       "1578611    [zup, there, !, learning, a, new, magic, trick]  \n",
       "1578612                        [zyklonic, showers, *evil*]  \n",
       "1578613  [ZZ, Top, â€“, I, Thank, You, ...@hawaiibuzz, ...  \n",
       "1578614  [zzz, time., Just, wish, my, love, could, B, n...  \n",
       "1578615  [zzz, twitter., good, day, today., got, a, lot...  \n",
       "1578616  [\"zzz's, time,, goodnight., http://plurk.com/p...  \n",
       "1578617  [Zzzz, lying, in, bed, watching, the, countrys...  \n",
       "1578618  [Zzzz..., Fuck, Ã¼, :, Zzzz..., Fuck, Ã¼, http...  \n",
       "1578619               [Zzzz...no, work, tomorrow..yayyy!!]  \n",
       "1578620  [ZZZZZ, time.., Tomorrow, will, be, a, busy, d...  \n",
       "1578621  [Zzzzz, want, to, sleep, but, at, sister's, in...  \n",
       "1578622           [Zzzzzz...., Finally!, Night, tweeters!]  \n",
       "1578623                [\"Zzzzzzz,, sleep, well, people, \"]  \n",
       "1578624      [ZzzZzZzzzZ..., wait, no, I, have, homework.]  \n",
       "1578625  [\"ZzZzzzZZZZzzz, meh,, what, am, I, doing, up,...  \n",
       "1578626                [\"Zzzzzzzzzzzzzzzzzzz,, I, wish, \"]  \n",
       "\n",
       "[1578627 rows x 4 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "size_list = [100, 110, 120, 130, 140, 150]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# convertion i = (x - 100)/10 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Res = np.zeros((10,6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# test \n",
    "# currently freezes \n",
    "size_s = 100\n",
    "window_s = 5 \n",
    "total_f1 = 0 \n",
    "kf = KFold(n_splits=10, shuffle=True)\n",
    "for train_indeces, test_indeces in kf.split(sentdat):\n",
    "    train_X = sentDf[\"SentimentText\"].iloc[train_indeces]\n",
    "    train_Y = sentDf[\"Sentiment\"].iloc[train_indeces]\n",
    "    test_X = sentDf[\"SentimentText\"].iloc[test_indeces]\n",
    "    test_Y = sentDf[\"Sentiment\"].iloc[test_indeces]\n",
    "    # Ns = int(1e5)\n",
    "    # training set word2vec \n",
    "    w2vM_train = gensim.models.Word2Vec(train_X, size=size_s, window=window_s)\n",
    "    tvecs_train = np.array([np.array([w2vM_train[t] if t in w2vM_train\n",
    "                                else np.zeros((size_s,))                               \n",
    "                            for t in twt]).mean(axis=0)\n",
    "                     for twt in train_X])\n",
    "    # test set word2vec \n",
    "    w2vM_test = gensim.models.Word2Vec(test_X, size=size_s, window=window_s)\n",
    "    tvecs_test = np.array([np.array([w2vM_test[t] if t in w2vM_test\n",
    "                                else np.zeros((size_s,))                               \n",
    "                            for t in twt]).mean(axis=0)\n",
    "                     for twt in train_X])\n",
    "   \n",
    "    # training\n",
    "    mlp = sklearn.neural_network.MLPClassifier()\n",
    "    mlp.fit(tvecs_train,train_Y)\n",
    "    yhat = mlp.predict(tvecs_test)\n",
    "    accu = np.mean(yhat == test_Y)\n",
    "    prec = np.mean(test_Y[yhat == 1])\n",
    "    recl = np.mean(yhat[test_Y == 1])\n",
    "    f1 = 2 * prec * recl / (prec + recl)\n",
    "    print(f1)\n",
    "    total_f1 += f1 \n",
    "avgf1 = total_f1 / 10 \n",
    "print(avgf1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-ebaa6a85dbb8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mu'time'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mu''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mu'for window_s in range(1, 11):\\n    for size_s in size_list:\\n        w2vM = gensim.models.Word2Vec(sentDf[\"SentimentText\"], size=size_s, window=window_s)\\n        Ns = int(1e5)\\n        tvecs = np.array([np.array([w2vM[t] if t in w2vM\\n                                else np.zeros((size_s,))\\n                            for t in twt]).mean(axis=0)\\n                 for twt in sentDf[\"SentimentText\"][:Ns]])\\n        # prediction vectors\\n        tvecs_predict = np.array([np.array([w2vM[t] if t in w2vM\\n                                else np.zeros((size_s,))\\n                            for t in twt]).mean(axis=0)\\n                 for twt in tweets[:Ns]])\\n        # training\\n        N = int(1e5)\\n        X = tvecs[:N]\\n        y = sentDf[\"Sentiment\"][:N].values\\n        ratio = 0.8\\n        tidx = np.random.rand(N) < ratio\\n        pidx = ~tidx\\n        mlp = sklearn.neural_network.MLPClassifier()\\n        mlp.fit(X[tidx],y[tidx])\\n        yhat = mlp.predict(X)\\n        accu = np.mean(yhat == y)\\n        prec = np.mean(y[yhat == 1])\\n        recl = np.mean(yhat[y == 1])\\n        f1 = 2 * prec * recl / (prec + recl)\\n        print(f1)\\n        # store the f1 score in the grid as the metrics\\n        Res[window_s-1, (size_s - 100)/10 ] = f1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/zeyuanxu/anaconda3/envs/py2env/lib/python2.7/site-packages/IPython/core/interactiveshell.pyc\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2115\u001b[0m             \u001b[0mmagic_arg_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar_expand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstack_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2116\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2117\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2118\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<decorator-gen-60>\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n",
      "\u001b[0;32m/home/zeyuanxu/anaconda3/envs/py2env/lib/python2.7/site-packages/IPython/core/magic.pyc\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/zeyuanxu/anaconda3/envs/py2env/lib/python2.7/site-packages/IPython/core/magics/execution.pyc\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[1;32m   1183\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1184\u001b[0m             \u001b[0mst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1185\u001b[0;31m             \u001b[0;32mexec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_ns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1186\u001b[0m             \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1187\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/home/zeyuanxu/anaconda3/envs/py2env/lib/python2.7/site-packages/gensim/models/word2vec.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, sentences, size, alpha, window, min_count, max_vocab_size, sample, seed, workers, min_alpha, sg, hs, negative, cbow_mean, hashfxn, iter, null_word, trim_rule, sorted_vocab, batch_words, compute_loss)\u001b[0m\n\u001b[1;32m    540\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGeneratorType\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    541\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"You can't pass a generator as the sentences argument. Try an iterator.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 542\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_vocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrim_rule\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrim_rule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    543\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_examples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus_count\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_alpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_alpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin_alpha\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/zeyuanxu/anaconda3/envs/py2env/lib/python2.7/site-packages/gensim/models/word2vec.pyc\u001b[0m in \u001b[0;36mbuild_vocab\u001b[0;34m(self, sentences, keep_raw_vocab, trim_rule, progress_per, update)\u001b[0m\n\u001b[1;32m    618\u001b[0m         \"\"\"\n\u001b[1;32m    619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscan_vocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprogress_per\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprogress_per\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrim_rule\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrim_rule\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# initial survey\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale_vocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeep_raw_vocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkeep_raw_vocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrim_rule\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrim_rule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupdate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# trim by min_count & precalculate downsampling\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinalize_vocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# build tables & arrays\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/zeyuanxu/anaconda3/envs/py2env/lib/python2.7/site-packages/gensim/models/word2vec.pyc\u001b[0m in \u001b[0;36mscale_vocab\u001b[0;34m(self, min_count, sample, dry_run, keep_raw_vocab, trim_rule, update)\u001b[0m\n\u001b[1;32m    731\u001b[0m                     \u001b[0mretain_total\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    732\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdry_run\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 733\u001b[0;31m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex2word\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    734\u001b[0m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex2word\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    735\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/zeyuanxu/anaconda3/envs/py2env/lib/python2.7/site-packages/gensim/models/keyedvectors.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time \n",
    "sentdat = sentDf[\"SentimentText\"]\n",
    "for window_s in range(1, 11):\n",
    "    for size_s in size_list:\n",
    "        total_f1 = 0 \n",
    "        kf = KFold(n_splits=10, shuffle=True)\n",
    "        for train_indeces, test_indeces in kf.split(sentdat):\n",
    "            train_X = sentDf[\"SentimentText\"].iloc[train_indeces]\n",
    "            train_Y = sentDf[\"Sentiment\"].iloc[train_indeces]\n",
    "            test_X = sentDf[\"SentimentText\"].iloc[test_indeces]\n",
    "            test_Y = sentDf[\"Sentiment\"].iloc[test_indeces]\n",
    "            Ns = int(1e5)\n",
    "            # training set word2vec \n",
    "            w2vM_train = gensim.models.Word2Vec(train_X, size=size_s, window=window_s)\n",
    "            tvecs_train = np.array([np.array([w2vM_train[t] if t in w2vM_train\n",
    "                                else np.zeros((size_s,))                               \n",
    "                            for t in twt]).mean(axis=0)\n",
    "                     for twt in sentDf[\"SentimentText\"][:Ns]])\n",
    "            # test set word2vec \n",
    "            w2vM_test = gensim.models.Word2Vec(test_X, size=size_s, window=window_s)\n",
    "            tvecs_test = np.array([np.array([w2vM_test[t] if t in w2vM_test\n",
    "                                else np.zeros((size_s,))                               \n",
    "                            for t in twt]).mean(axis=0)\n",
    "                     for twt in sentDf[\"SentimentText\"][:Ns]])\n",
    "   \n",
    "            # training\n",
    "            mlp = sklearn.neural_network.MLPClassifier()\n",
    "            mlp.fit(tvecs_train,train_Y)\n",
    "            yhat = mlp.predict(tvecs_test)\n",
    "            accu = np.mean(yhat == test_Y)\n",
    "            prec = np.mean(test_Y[yhat == 1])\n",
    "            recl = np.mean(yhat[test_Y == 1])\n",
    "            f1 = 2 * prec * recl / (prec + recl)\n",
    "            print(f1)\n",
    "            total_f1 += f1 \n",
    "        avgf1 = total_f1 / 10 \n",
    "    # store the f1 score in the grid as the metrics\n",
    "    Res[window_s-1, (size_s - 100)/10 ] = avgf1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.81004103,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
