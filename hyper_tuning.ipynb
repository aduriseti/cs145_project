{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import gensim\n",
    "import sklearn\n",
    "from sklearn import ensemble,svm,neural_network,discriminant_analysis\n",
    "from sklearn.metrics import roc_curve,auc,precision_recall_curve\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "with open(\"sent_corpus.csv\", \"r\") as sent_file:\n",
    "    lines = sent_file.read().split(\"\\n\")\n",
    "\n",
    "rows = [line.split(\",\") for line in lines if line]\n",
    "rows = [row[:3] + [\",\".join(row[3:])] for row in rows]\n",
    "# remove document start character\n",
    "rows[0][0] = rows[0][0][1:]\n",
    "sentDf_cols = ['ItemID', 'Sentiment', 'SentimentSource', 'SentimentText']\n",
    "sentDf = pd.DataFrame(rows[1:],columns=sentDf_cols)\n",
    "# print(sentDf.columns.values)\n",
    "sentDf[[\"ItemID\",\"Sentiment\"]] = sentDf[[\"ItemID\",\"Sentiment\"]].astype(int)\n",
    "sentDf[\"SentimentText\"] = sentDf[\"SentimentText\"].apply(lambda text: text.split())\n",
    "## -*- coding: utf-8 -*-\n",
    "curdir = os.getcwd()\n",
    "tweet_path = curdir + \"/crawler_v1/code/tweettext.txt\"\n",
    "# print(tweet_path)\n",
    "\n",
    "tweets = []\n",
    "tweet_file = open(tweet_path, \"rb\")\n",
    "for line in tweet_file:\n",
    "#    print(line)\n",
    "#    line = line_pre.decode('utf-8')\n",
    "    tweet = \"\"\n",
    "    if(line[:2] == \"RT\"):\n",
    "        char_loc = 0\n",
    "        for char in line:\n",
    "            char_loc += 1\n",
    "            if(char==\":\"):\n",
    "                break\n",
    "\n",
    "        tweet = line[-(len(line)-char_loc):]\n",
    "    else:\n",
    "        tweet = line\n",
    "    tweets.append(tweet)\n",
    "\n",
    "#[tweet.decode('utf-8') for tweet in tweets]\n",
    "# print(tweets)\n",
    "# print(\"done\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1-D searches on hyperparameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ItemID</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>SentimentSource</th>\n",
       "      <th>SentimentText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Sentiment140</td>\n",
       "      <td>[is, so, sad, for, my, APL, friend.............]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>Sentiment140</td>\n",
       "      <td>[I, missed, the, New, Moon, trailer...]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>Sentiment140</td>\n",
       "      <td>[omg, its, already, 7:30, :O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>Sentiment140</td>\n",
       "      <td>[.., Omgaga., Im, sooo, im, gunna, CRy., I've,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>Sentiment140</td>\n",
       "      <td>[i, think, mi, bf, is, cheating, on, me!!!, T_T]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>Sentiment140</td>\n",
       "      <td>[or, i, just, worry, too, much?]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>Sentiment140</td>\n",
       "      <td>[Juuuuuuuuuuuuuuuuussssst, Chillin!!]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>Sentiment140</td>\n",
       "      <td>[Sunny, Again, Work, Tomorrow, :-|, TV, Tonight]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>Sentiment140</td>\n",
       "      <td>[handed, in, my, uniform, today, ., i, miss, y...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>Sentiment140</td>\n",
       "      <td>[hmmmm...., i, wonder, how, she, my, number, @-)]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ItemID  Sentiment SentimentSource  \\\n",
       "0       1          0    Sentiment140   \n",
       "1       2          0    Sentiment140   \n",
       "2       3          1    Sentiment140   \n",
       "3       4          0    Sentiment140   \n",
       "4       5          0    Sentiment140   \n",
       "5       6          0    Sentiment140   \n",
       "6       7          1    Sentiment140   \n",
       "7       8          0    Sentiment140   \n",
       "8       9          1    Sentiment140   \n",
       "9      10          1    Sentiment140   \n",
       "\n",
       "                                       SentimentText  \n",
       "0   [is, so, sad, for, my, APL, friend.............]  \n",
       "1            [I, missed, the, New, Moon, trailer...]  \n",
       "2                      [omg, its, already, 7:30, :O]  \n",
       "3  [.., Omgaga., Im, sooo, im, gunna, CRy., I've,...  \n",
       "4   [i, think, mi, bf, is, cheating, on, me!!!, T_T]  \n",
       "5                   [or, i, just, worry, too, much?]  \n",
       "6              [Juuuuuuuuuuuuuuuuussssst, Chillin!!]  \n",
       "7   [Sunny, Again, Work, Tomorrow, :-|, TV, Tonight]  \n",
       "8  [handed, in, my, uniform, today, ., i, miss, y...  \n",
       "9  [hmmmm...., i, wonder, how, she, my, number, @-)]  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentDf.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "size_list = [100, 110, 120, 130, 140, 150]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# convertion i = (x - 100)/10 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Res = np.zeros((10,6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "0.681203713994\n",
      "2\n",
      "0.579511057872\n",
      "3\n",
      "0.656619721451\n",
      "4\n",
      "0.543026947814\n",
      "5\n",
      "0.65166750766\n",
      "0.622405789758\n",
      "CPU times: user 1h 1min 32s, sys: 1min 4s, total: 1h 2min 36s\n",
      "Wall time: 31min 58s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# test effect of \n",
    "count=0 \n",
    "sentdat = sentDf[\"SentimentText\"]\n",
    "size_s = 100\n",
    "window_s = 5 \n",
    "total_f1 = 0 \n",
    "kf = KFold(n_splits=5, shuffle=True)\n",
    "for train_indeces, test_indeces in kf.split(sentdat):\n",
    "    train_X = sentDf[\"SentimentText\"].iloc[train_indeces]\n",
    "    train_Y = sentDf[\"Sentiment\"].iloc[train_indeces]\n",
    "    test_X = sentDf[\"SentimentText\"].iloc[test_indeces]\n",
    "    test_Y = sentDf[\"Sentiment\"].iloc[test_indeces]\n",
    "    # Ns = int(1e5)\n",
    "    # training set word2vec \n",
    "    w2vM1 = gensim.models.Word2Vec(train_X, size=size_s, window=window_s)\n",
    "    tvecs_train = np.array([np.array([w2vM1[t] if t in w2vM1\n",
    "                                else np.zeros((size_s,))                               \n",
    "                            for t in twt]).mean(axis=0)\n",
    "                     for twt in train_X])\n",
    "    # test set word2vec \n",
    "    w2vM2 = gensim.models.Word2Vec(test_X, size=size_s, window=window_s)\n",
    "    tvecs_test = np.array([np.array([w2vM2[t] if t in w2vM2\n",
    "                                else np.zeros((size_s,))                               \n",
    "                            for t in twt]).mean(axis=0)\n",
    "                     for twt in test_X])\n",
    "   \n",
    "    # training\n",
    "    mlp = sklearn.neural_network.MLPClassifier()\n",
    "    mlp.fit(tvecs_train,train_Y)\n",
    "    yhat = mlp.predict(tvecs_test)\n",
    "    accu = np.mean(yhat == test_Y)\n",
    "    prec = np.mean(test_Y[yhat == 1])\n",
    "    recl = np.mean(yhat[test_Y == 1])\n",
    "    f1 = 2 * prec * recl / (prec + recl)\n",
    "    count+=1 \n",
    "    print(count)\n",
    "    print(f1)\n",
    "    total_f1 += f1 \n",
    "avgf1 = total_f1 / 5 \n",
    "print(avgf1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.663737209231\n",
      "0.538535024603\n",
      "0.670740934133\n",
      "0.631841331969\n",
      "0.638166431339\n",
      "0.628604186255\n",
      "0.493301658054\n",
      "0.585393313771\n",
      "0.485609348365\n",
      "0.687707596008\n",
      "0.664056704622\n",
      "0.583213724164\n",
      "0.582441874745\n",
      "0.651941120673\n",
      "0.608930424287\n",
      "0.545765688344\n",
      "0.658151959279\n",
      "0.609446213466\n",
      "0.674993293424\n",
      "0.569633470663\n",
      "0.548441986199\n",
      "0.666898814463\n",
      "0.659139695858\n",
      "0.623821452122\n",
      "0.579081649826\n",
      "0.671334701275\n",
      "0.510108444115\n",
      "0.469849001048\n",
      "0.658212037438\n",
      "0.57771716674\n",
      "0.66177614817\n",
      "0.540865730673\n",
      "0.633191755617\n",
      "0.620580691095\n",
      "0.517409068965\n",
      "0.594764678904\n",
      "0.677866061294\n",
      "0.436807297804\n",
      "0.46907341269\n",
      "0.632108133744\n",
      "0.534687150012\n",
      "0.550108411109\n",
      "0.427082826765\n",
      "0.633430252726\n",
      "0.631802803287\n",
      "0.47831762263\n",
      "0.487778873364\n",
      "0.531682475754\n",
      "0.599812490985\n",
      "0.550565627954\n",
      "0.544030352677\n",
      "0.406612851735\n",
      "0.51267209372\n",
      "0.522738683415\n",
      "0.505502410715\n",
      "0.483879166498\n",
      "0.553865723853\n",
      "0.416524384088\n",
      "0.327573645243\n",
      "0.457469066079\n",
      "CPU times: user 10h 19min 17s, sys: 12min 34s, total: 10h 31min 51s\n",
      "Wall time: 5h 23min 56s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "#examine the effect of window size first \n",
    "res1 = [] \n",
    "sentdat = sentDf[\"SentimentText\"]\n",
    "for window_s in range(1, 11):\n",
    "    #for size_s in size_list:\n",
    "    size_s = 100 \n",
    "    total_f1 = 0 \n",
    "    kf = KFold(n_splits=5, shuffle=True)\n",
    "    for train_indeces, test_indeces in kf.split(sentdat):\n",
    "        train_X = sentDf[\"SentimentText\"].iloc[train_indeces]\n",
    "        train_Y = sentDf[\"Sentiment\"].iloc[train_indeces]\n",
    "        test_X = sentDf[\"SentimentText\"].iloc[test_indeces]\n",
    "        test_Y = sentDf[\"Sentiment\"].iloc[test_indeces]\n",
    "            # Ns = int(1e5)\n",
    "            # training set word2vec \n",
    "        w2vM1 = gensim.models.Word2Vec(train_X, size=size_s, window=window_s)\n",
    "        tvecs_train = np.array([np.array([w2vM1[t] if t in w2vM1\n",
    "                                            else np.zeros((size_s,))                               \n",
    "                                            for t in twt]).mean(axis=0)\n",
    "                                            for twt in train_X])\n",
    "            # test set word2vec \n",
    "        w2vM2 = gensim.models.Word2Vec(test_X, size=size_s, window=window_s)\n",
    "        tvecs_test = np.array([np.array([w2vM2[t] if t in w2vM2\n",
    "                                    else np.zeros((size_s,))                               \n",
    "                                    for t in twt]).mean(axis=0)\n",
    "                                    for twt in test_X])\n",
    "   \n",
    "            # training\n",
    "        mlp = sklearn.neural_network.MLPClassifier()\n",
    "        mlp.fit(tvecs_train,train_Y)\n",
    "        yhat = mlp.predict(tvecs_test)\n",
    "        accu = np.mean(yhat == test_Y)\n",
    "        prec = np.mean(test_Y[yhat == 1])\n",
    "        recl = np.mean(yhat[test_Y == 1])\n",
    "        f1 = 2 * prec * recl / (prec + recl)\n",
    "        print(f1)\n",
    "        total_f1 += f1 \n",
    "    avgf1 = total_f1 / 5 \n",
    "    print(avgf1)\n",
    "        # store the f1 score in the grid as the metrics\n",
    "    res1.append(avgf1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.6286041862550471,\n",
       " 0.58321372416399864,\n",
       " 0.609446213465622,\n",
       " 0.62382145212159301,\n",
       " 0.57771716674034723,\n",
       " 0.59476467890394968,\n",
       " 0.55010841110874997,\n",
       " 0.53168247575417127,\n",
       " 0.52273868341450636,\n",
       " 0.45746906607924875]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.583316239143\n",
      "0.601730913933\n",
      "0.571610200031\n",
      "0.642035005679\n",
      "0.656939492252\n",
      "0.536194690559\n",
      "CPU times: user 7h 57min 31s, sys: 9min 50s, total: 8h 7min 21s\n",
      "Wall time: 4h 11min 11s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "#examine the effect of size along \n",
    "res2 = [] \n",
    "sentdat = sentDf[\"SentimentText\"]\n",
    "window_s = 1 \n",
    "for size_s in size_list: \n",
    "    total_f1 = 0 \n",
    "    kf = KFold(n_splits=5, shuffle=True)\n",
    "    for train_indeces, test_indeces in kf.split(sentdat):\n",
    "        train_X = sentDf[\"SentimentText\"].iloc[train_indeces]\n",
    "        train_Y = sentDf[\"Sentiment\"].iloc[train_indeces]\n",
    "        test_X = sentDf[\"SentimentText\"].iloc[test_indeces]\n",
    "        test_Y = sentDf[\"Sentiment\"].iloc[test_indeces]\n",
    "            # Ns = int(1e5)\n",
    "            # training set word2vec \n",
    "        w2vM1 = gensim.models.Word2Vec(train_X, size=size_s, window=window_s)\n",
    "        tvecs_train = np.array([np.array([w2vM1[t] if t in w2vM1\n",
    "                                            else np.zeros((size_s,))                               \n",
    "                                            for t in twt]).mean(axis=0)\n",
    "                                            for twt in train_X])\n",
    "            # test set word2vec \n",
    "        w2vM2 = gensim.models.Word2Vec(test_X, size=size_s, window=window_s)\n",
    "        tvecs_test = np.array([np.array([w2vM2[t] if t in w2vM2\n",
    "                                    else np.zeros((size_s,))                               \n",
    "                                    for t in twt]).mean(axis=0)\n",
    "                                    for twt in test_X])\n",
    "   \n",
    "            # training\n",
    "        mlp = sklearn.neural_network.MLPClassifier()\n",
    "        mlp.fit(tvecs_train,train_Y)\n",
    "        yhat = mlp.predict(tvecs_test)\n",
    "        accu = np.mean(yhat == test_Y)\n",
    "        prec = np.mean(test_Y[yhat == 1])\n",
    "        recl = np.mean(yhat[test_Y == 1])\n",
    "        f1 = 2 * prec * recl / (prec + recl)\n",
    "       # print(f1)\n",
    "        total_f1 += f1 \n",
    "    avgf1 = total_f1 / 5 \n",
    "    print(avgf1)\n",
    "        # store the f1 score in the grid as the metrics\n",
    "    res2.append(avgf1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.58331623914256769,\n",
       " 0.60173091393263833,\n",
       " 0.57161020003090379,\n",
       " 0.64203500567903926,\n",
       " 0.65693949225247883,\n",
       " 0.53619469055893942]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "140\n"
     ]
    }
   ],
   "source": [
    "max_size_s = res2.index(max(res2))\n",
    "print(size_list[max_size_s])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.528952238453\n",
      "0.648368169868\n",
      "0.655557036587\n",
      "0.361017480227\n",
      "0.494371091608\n",
      "0.537653203349\n",
      "0.670917958205\n",
      "0.675034605239\n",
      "0.635727453439\n",
      "0.570370968215\n",
      "0.623449069719\n",
      "0.635100010964\n",
      "0.678533071898\n",
      "0.576089504088\n",
      "0.294803248476\n",
      "0.659547696842\n",
      "0.662212515733\n",
      "0.574237207407\n",
      "0.586663439103\n",
      "0.626016308341\n",
      "0.369858088931\n",
      "0.67184562947\n",
      "0.674605077785\n",
      "0.585797708726\n",
      "CPU times: user 5h 4min 33s, sys: 7min 2s, total: 5h 11min 36s\n",
      "Wall time: 2h 40min 59s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "# candidate range: wind_s = 1 or 4, size = 130 or 40 \n",
    "# find the f1 score for cells intersecting the best candidate \n",
    "wind_cand = [1, 4]\n",
    "size_cand = [130, 140]\n",
    "#examine the effect of window size first \n",
    "res3 = [] \n",
    "sentdat = sentDf[\"SentimentText\"]\n",
    "for window_s in wind_cand:\n",
    "    for size_s in size_cand:  \n",
    "        total_f1 = 0 \n",
    "        kf = KFold(n_splits=5, shuffle=True)\n",
    "        for train_indeces, test_indeces in kf.split(sentdat):\n",
    "            train_X = sentDf[\"SentimentText\"].iloc[train_indeces]\n",
    "            train_Y = sentDf[\"Sentiment\"].iloc[train_indeces]\n",
    "            test_X = sentDf[\"SentimentText\"].iloc[test_indeces]\n",
    "            test_Y = sentDf[\"Sentiment\"].iloc[test_indeces]\n",
    "            # Ns = int(1e5)\n",
    "            # training set word2vec \n",
    "            w2vM1 = gensim.models.Word2Vec(train_X, size=size_s, window=window_s)\n",
    "            tvecs_train = np.array([np.array([w2vM1[t] if t in w2vM1\n",
    "                                            else np.zeros((size_s,))                               \n",
    "                                            for t in twt]).mean(axis=0)\n",
    "                                            for twt in train_X])\n",
    "            # test set word2vec \n",
    "            w2vM2 = gensim.models.Word2Vec(test_X, size=size_s, window=window_s)\n",
    "            tvecs_test = np.array([np.array([w2vM2[t] if t in w2vM2\n",
    "                                    else np.zeros((size_s,))                               \n",
    "                                    for t in twt]).mean(axis=0)\n",
    "                                    for twt in test_X])\n",
    "   \n",
    "            # training\n",
    "            mlp = sklearn.neural_network.MLPClassifier()\n",
    "            mlp.fit(tvecs_train,train_Y)\n",
    "            yhat = mlp.predict(tvecs_test)\n",
    "            accu = np.mean(yhat == test_Y)\n",
    "            prec = np.mean(test_Y[yhat == 1])\n",
    "            recl = np.mean(yhat[test_Y == 1])\n",
    "            f1 = 2 * prec * recl / (prec + recl)\n",
    "            print(f1)\n",
    "            total_f1 += f1 \n",
    "        avgf1 = total_f1 / 5 \n",
    "        print(avgf1)\n",
    "        # store the f1 score in the grid as the metrics\n",
    "        res3.append(avgf1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.53765320334871325,\n",
       " 0.63510001096353474,\n",
       " 0.57423720740726847,\n",
       " 0.58579770872625048]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 1 and 140 are the best values for window size and size "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.476345236134\n",
      "0.471677950478\n",
      "0.53690397829\n",
      "0.587296214155\n",
      "0.58041945421\n",
      "0.450108135444\n",
      "0.602641403302\n",
      "0.555231877584\n",
      "0.574968157667\n",
      "0.587839002012\n",
      "CPU times: user 10h 31min 23s, sys: 12min 36s, total: 10h 43min 59s\n",
      "Wall time: 5h 30min 16s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "# testing more hyperparameters \n",
    "# add two more hyperparameters: min_count = [1,2,3,4,5,6,7,8,9]\n",
    "# iter = [1,2,3,4,5,6,7,8,9]\n",
    "#examine the effect of mincount alone  \n",
    "res4 = [] \n",
    "sentdat = sentDf[\"SentimentText\"]\n",
    "for min_count in range(1,11): \n",
    "    total_f1 = 0 \n",
    "    kf = KFold(n_splits=5, shuffle=True)\n",
    "    for train_indeces, test_indeces in kf.split(sentdat):\n",
    "        train_X = sentDf[\"SentimentText\"].iloc[train_indeces]\n",
    "        train_Y = sentDf[\"Sentiment\"].iloc[train_indeces]\n",
    "        test_X = sentDf[\"SentimentText\"].iloc[test_indeces]\n",
    "        test_Y = sentDf[\"Sentiment\"].iloc[test_indeces]\n",
    "            # Ns = int(1e5)\n",
    "            # training set word2vec \n",
    "        w2vM1 = gensim.models.Word2Vec(train_X, min_count=min_count)\n",
    "        tvecs_train = np.array([np.array([w2vM1[t] if t in w2vM1\n",
    "                                            else np.zeros((100,))                               \n",
    "                                            for t in twt]).mean(axis=0)\n",
    "                                            for twt in train_X])\n",
    "            # test set word2vec \n",
    "        w2vM2 = gensim.models.Word2Vec(test_X, min_count=min_count)\n",
    "        tvecs_test = np.array([np.array([w2vM2[t] if t in w2vM2\n",
    "                                    else np.zeros((100,))                               \n",
    "                                    for t in twt]).mean(axis=0)\n",
    "                                    for twt in test_X])\n",
    "   \n",
    "            # training\n",
    "        mlp = sklearn.neural_network.MLPClassifier()\n",
    "        mlp.fit(tvecs_train,train_Y)\n",
    "        yhat = mlp.predict(tvecs_test)\n",
    "        accu = np.mean(yhat == test_Y)\n",
    "        prec = np.mean(test_Y[yhat == 1])\n",
    "        recl = np.mean(yhat[test_Y == 1])\n",
    "        f1 = 2 * prec * recl / (prec + recl)\n",
    "       # print(f1)\n",
    "        total_f1 += f1 \n",
    "    avgf1 = total_f1 / 5 \n",
    "    print(avgf1)\n",
    "        # store the f1 score in the grid as the metrics\n",
    "    res4.append(avgf1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.226380788676\n",
      "0.422943963953\n",
      "0.441420007169\n",
      "0.562868655333\n",
      "0.585407691429\n",
      "0.544513815189\n",
      "0.604352710173\n",
      "0.5884048094\n",
      "0.61012185828\n",
      "0.571130876475\n",
      "CPU times: user 11h 6min 30s, sys: 13min 5s, total: 11h 19min 35s\n",
      "Wall time: 5h 46min 24s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "# testing more hyperparameters \n",
    "# add two more hyperparameters: min_count = [1,2,3,4,5,6,7,8,9]\n",
    "# iter = [1,2,3,4,5,6,7,8,9]\n",
    "#examine the effect of size along \n",
    "res5 = [] \n",
    "sentdat = sentDf[\"SentimentText\"]\n",
    "for iteration in range(1,11): \n",
    "    total_f1 = 0 \n",
    "    kf = KFold(n_splits=5, shuffle=True)\n",
    "    for train_indeces, test_indeces in kf.split(sentdat):\n",
    "        train_X = sentDf[\"SentimentText\"].iloc[train_indeces]\n",
    "        train_Y = sentDf[\"Sentiment\"].iloc[train_indeces]\n",
    "        test_X = sentDf[\"SentimentText\"].iloc[test_indeces]\n",
    "        test_Y = sentDf[\"Sentiment\"].iloc[test_indeces]\n",
    "            # Ns = int(1e5)\n",
    "            # training set word2vec \n",
    "        w2vM1 = gensim.models.Word2Vec(train_X, iter=iteration)\n",
    "        tvecs_train = np.array([np.array([w2vM1[t] if t in w2vM1\n",
    "                                            else np.zeros((100,))                               \n",
    "                                            for t in twt]).mean(axis=0)\n",
    "                                            for twt in train_X])\n",
    "            # test set word2vec \n",
    "        w2vM2 = gensim.models.Word2Vec(test_X, iter=iteration)\n",
    "        tvecs_test = np.array([np.array([w2vM2[t] if t in w2vM2\n",
    "                                    else np.zeros((100,))                               \n",
    "                                    for t in twt]).mean(axis=0)\n",
    "                                    for twt in test_X])\n",
    "   \n",
    "            # training\n",
    "        mlp = sklearn.neural_network.MLPClassifier()\n",
    "        mlp.fit(tvecs_train,train_Y)\n",
    "        yhat = mlp.predict(tvecs_test)\n",
    "        accu = np.mean(yhat == test_Y)\n",
    "        prec = np.mean(test_Y[yhat == 1])\n",
    "        recl = np.mean(yhat[test_Y == 1])\n",
    "        f1 = 2 * prec * recl / (prec + recl)\n",
    "       # print(f1)\n",
    "        total_f1 += f1 \n",
    "    avgf1 = total_f1 / 5 \n",
    "    print(avgf1)\n",
    "        # store the f1 score in the grid as the metrics\n",
    "    res5.append(avgf1) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Summary for 1-D search of hyperparameters\n",
    "hyperparameters tested are: \n",
    "* Size: candidates are picked from the list [100,110,120,130,140,150]. and the best result is when size=100 \n",
    "* Window size: candidates are picked from 1 to 10, and the best result is wind_size = 1 \n",
    "* min_count: candidates are picked from 1 to 10, and the best result is when min_count = 7 \n",
    "* iter: candidates are picked from 1 to 10, and the best result is for iter=9\n",
    "\n",
    "The rest to test is around the neighborhood of the tentative optimal point (140, 1, 7, 9). The neighbor contains the points: \n",
    "(130 or 140 or 150, 1 or 2, 6 or 7 or 8, 8 or 9 or 10), a total of 54 points.\n",
    "\n",
    "Since 10 cases take around 6 hours to test, 54 cases take more than 1 day to finish. Therefore, need another reduction in candidates: for 1-D scan in wind_size, 2 performs much worse, so omit 2 and take the second best performer 4. for 1-D scan on size, the result from 130 is much better than 150, so omit 150. For 1-D scan on min_count, 6 performs almost the worst in the candidates, so omit 6. For 1-D scan on iteration, since the variation is low, omit 8. The remaining candidates are: \n",
    "(130 or 140, 1 or 4, 7 or 8, 9 or 10), a total of 16 points. \n",
    "\n",
    "Moreoever, when setting value to 130, the warning of \"setting size to a multiple of 4 for better performance\" is generated, therefore, size candidates are set to the cloest multiple of 4 to 130 in the range (130,140), and results are (132, 140) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The neighbor list\n",
    "cand_list = [[132, 1, 7, 9], [132, 1, 7, 10], [132, 1, 8, 9], [132, 1, 8, 10],\n",
    "             [132, 4, 7, 9], [132, 4, 7, 10], [132, 4, 8, 9], [132, 4, 8, 10],\n",
    "             [140, 1, 7, 9], [140, 1, 7, 10], [140, 1, 8, 9], [140, 1, 8, 10],\n",
    "             [140, 4, 7, 9], [140, 4, 7, 10], [140, 4, 8, 9], [140, 4, 8, 10]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.63894002039\n",
      "0.67790814957\n",
      "0.648225117446\n",
      "0.647054412866\n",
      "0.58940117464\n",
      "0.6443832454\n",
      "0.612732949165\n",
      "0.645904458341\n",
      "0.661916432364\n",
      "0.615625603522\n",
      "0.664619296601\n",
      "0.665382919372\n",
      "0.64680585469\n",
      "0.633735977261\n",
      "0.579322116869\n",
      "0.661123778066\n",
      "CPU times: user 23h 41min 50s, sys: 30min 38s, total: 1d 12min 29s\n",
      "Wall time: 11h 44min 24s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "final_res = [] \n",
    "sentdat = sentDf[\"SentimentText\"]\n",
    "for elem_tuple in cand_list: \n",
    "    total_f1 = 0 \n",
    "    kf = KFold(n_splits=5, shuffle=True)\n",
    "    for train_indeces, test_indeces in kf.split(sentdat):\n",
    "        train_X = sentDf[\"SentimentText\"].iloc[train_indeces]\n",
    "        train_Y = sentDf[\"Sentiment\"].iloc[train_indeces]\n",
    "        test_X = sentDf[\"SentimentText\"].iloc[test_indeces]\n",
    "        test_Y = sentDf[\"Sentiment\"].iloc[test_indeces]\n",
    "        size = elem_tuple[0]\n",
    "        wind_size = elem_tuple[1]\n",
    "        min_count = elem_tuple[2]\n",
    "        iteration = elem_tuple[3]\n",
    "        #print(\"before the word2vec call\")\n",
    "        w2vM1 = gensim.models.Word2Vec(train_X, size=size, window=wind_size, min_count=min_count, iter=iteration)\n",
    "        tvecs_train = np.array([np.array([w2vM1[t] if t in w2vM1\n",
    "                                            else np.zeros((size,))                               \n",
    "                                            for t in twt]).mean(axis=0)\n",
    "                                            for twt in train_X])\n",
    "            # test set word2vec \n",
    "        w2vM2 = gensim.models.Word2Vec(test_X, size=size, window=wind_size, min_count=min_count, iter=iteration)\n",
    "        tvecs_test = np.array([np.array([w2vM2[t] if t in w2vM2\n",
    "                                    else np.zeros((size,))                               \n",
    "                                    for t in twt]).mean(axis=0)\n",
    "                                    for twt in test_X])\n",
    "   \n",
    "            # training\n",
    "        mlp = sklearn.neural_network.MLPClassifier()\n",
    "        mlp.fit(tvecs_train,train_Y)\n",
    "        yhat = mlp.predict(tvecs_test)\n",
    "        accu = np.mean(yhat == test_Y)\n",
    "        prec = np.mean(test_Y[yhat == 1])\n",
    "        recl = np.mean(yhat[test_Y == 1])\n",
    "        f1 = 2 * prec * recl / (prec + recl)\n",
    "       # print(f1)\n",
    "        total_f1 += f1 \n",
    "    avgf1 = total_f1 / 5 \n",
    "    print(avgf1)\n",
    "        # store the f1 score in the grid as the metrics\n",
    "    final_res.append(avgf1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
