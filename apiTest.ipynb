{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Data collection\n",
    "\n",
    "### Load packages/set globals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tweepy\n",
    "consumer_key = \"51MI8RrYmzO4btCKG4Qb5uqAa\"\n",
    "consumer_secret = \"ajpPv3Ag0NvMEQLBIwiPyDyU78BbLZn8IS1gTba4x9ZOHNPMNM\"\n",
    "access_token = \"3004471069-VDbNpT9NO0QOtiqKZXkoH5Flv4MArCflIYImXjn\"\n",
    "access_token_secret = \"sP6KMjPZXxYAnaae8bOiauLjCVnx8bzWkBk4KU1iZBxdl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Creating the authentication object\n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "# Setting your access token and secret\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "# Creating the API object while passing in auth information\n",
    "api = tweepy.API(auth) \n",
    "\n",
    "n = 3 # Tweet count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Basic API functionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seattle is the Second Coolest City in America. Wait, Second?! https://t.co/jVxey0MVmm https://t.co/qwWleeQcKP\n",
      "Opioid addicts turn to Kratom for relief https://t.co/yAXITKLdBB https://t.co/00O5qflBxw\n",
      "Experience aplenty: Seattle returns 14 players, including eight starters, from last year's MLS Cup-winning team.… https://t.co/9z42XmGkX9\n"
     ]
    }
   ],
   "source": [
    "# Print tweets from my home timeline\n",
    "for status in tweepy.Cursor(api.home_timeline).items(n):\n",
    "    # Process a single status\n",
    "    print(status.text)\n",
    "#     print(status._json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When we discard a person’s accumulated possessions, we are throwing out the record of a life https://t.co/hTEyjTS5lJ\n",
      "Here's where to stream every major '90s comedy https://t.co/25ASoE5QiI\n",
      "So this happened in our comments section today https://t.co/oOoSEAf0zM\n"
     ]
    }
   ],
   "source": [
    "# Getting tweets from the New York Times user\n",
    "for tweet in api.user_timeline(id=\"nytimes\", count=n):\n",
    "    # printing the text stored inside the tweet object\n",
    "    print(tweet.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QuinnXCII Tweeted: @KyleDeMarco everything is in stock and shipping! Apologize for the confusion\n",
      "nguyenh49266651 Tweeted: (Download) STOCK ROM TCL S500 (MT6577): https://t.co/mhZ1ncl4al via @YouTube\n",
      "jayi_wang Tweeted: RT @jayi_wang: https://t.co/KuoZWdWeUT\n",
      "Real Time Quote &amp; News\n"
     ]
    }
   ],
   "source": [
    "# get tweets by keyword\n",
    "for tweet in api.search(q=\"stock\", lang=\"en\")[:n]:\n",
    "    # printing the text stored inside the tweet object\n",
    "    print(tweet.user.screen_name,\"Tweeted:\",tweet.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Tweet processing\n",
    "\n",
    "## Tokenization\n",
    "\n",
    "### Default Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['@', 'mbobbish', '@', 'chrisco_sr', '@', 'POTUS', '@', 'WhiteHouse', 'Neil', 'Gorsuch', 'getting', 'confirmed', 'is', 'good', '.', 'Economy', 'is', 'getting', 'better', ',', 'as', 'the…', 'https', ':', '//t.co/Rlc0docyIE']\n"
     ]
    }
   ],
   "source": [
    "# note a default tokenizer doesn't do a good job\n",
    "tweets = api.search(q=\"stock\", lang=\"en\")\n",
    "tweet  = tweets[0].text\n",
    "print(word_tokenize(tweet))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tweet specific tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    " \n",
    "emoticons_str = r\"\"\"\n",
    "    (?:\n",
    "        [:=;] # Eyes\n",
    "        [oO\\-]? # Nose (optional)\n",
    "        [D\\)\\]\\(\\]/\\\\OpP] # Mouth\n",
    "    )\"\"\"\n",
    " \n",
    "regex_str = [\n",
    "    emoticons_str,\n",
    "    r'<[^>]+>', # HTML tags\n",
    "    r'(?:@[\\w_]+)', # @-mentions\n",
    "    r\"(?:\\#+[\\w_]+[\\w\\'_\\-]*[\\w_]+)\", # hash-tags\n",
    "    r'http[s]?://(?:[a-z]|[0-9]|[$-_@.&amp;+]|[!*\\(\\),]|(?:%[0-9a-f][0-9a-f]))+', # URLs\n",
    " \n",
    "    r'(?:(?:\\d+,?)+(?:\\.?\\d+)?)', # numbers\n",
    "    r\"(?:[a-z][a-z'\\-_]+[a-z])\", # words with - and '\n",
    "    r'(?:[\\w_]+)', # other words\n",
    "    r'(?:\\S)' # anything else\n",
    "]\n",
    "    \n",
    "tokens_re = re.compile(r'('+'|'.join(regex_str)+')', re.VERBOSE | re.IGNORECASE)\n",
    "emoticon_re = re.compile(r'^'+emoticons_str+'$', re.VERBOSE | re.IGNORECASE)\n",
    " \n",
    "def tokenize(s):\n",
    "    return tokens_re.findall(s)\n",
    " \n",
    "def preprocess(s, lowercase=False):\n",
    "    tokens = tokenize(s)\n",
    "    if lowercase:\n",
    "        tokens = [token if emoticon_re.search(token) else token.lower() for token in tokens]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['@mbobbish', '@chrisco_sr', '@POTUS', '@WhiteHouse', 'Neil', 'Gorsuch', 'getting', 'confirmed', 'is', 'good', '.', 'Economy', 'is', 'getting', 'better', ',', 'as', 'the', '…', 'https://t.co/Rlc0docyIE']\n"
     ]
    }
   ],
   "source": [
    "print(preprocess(tweet))\n",
    "# ['RT', '@marcobonzanini', ':', 'just', 'an', 'example', '!', ':D', 'http://example.com', '#NLP']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stop terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import string\n",
    " \n",
    "punctuation = list(string.punctuation)\n",
    "stop = stopwords.words('english') + punctuation + ['rt', 'via']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@mbobbish @chrisco_sr @POTUS @WhiteHouse Neil Gorsuch getting confirmed is good.  Economy is getting better, as the… https://t.co/Rlc0docyIE\n",
      "[('getting', 2), ('Economy', 1), ('Gorsuch', 1), ('confirmed', 1), ('Neil', 1)]\n",
      "@KevinSparkz Is there a stock for the HSD glitch?\n",
      "[('getting', 2), ('Economy', 1), ('Is', 1), ('Gorsuch', 1), ('stock', 1)]\n",
      "Me: I don't put much stock in a man that hasn't coached in 10 years.\n",
      "My wife: UT hasn't had a good coach in 10 years...\n",
      "#Truth  #GRUMORS\n",
      "[(\"hasn't\", 2), ('10', 2), ('getting', 2), ('years', 2), ('good', 2)]\n"
     ]
    }
   ],
   "source": [
    "import operator \n",
    "import json\n",
    "from collections import Counter\n",
    " \n",
    "count_all = Counter()\n",
    "for tweet in tweets[:3]:\n",
    "    # Create a list with all the terms\n",
    "    terms_all = [term for term in preprocess(tweet.text) if term not in stop]\n",
    "    # Update the counter\n",
    "    count_all.update(terms_all)\n",
    "    # Print the first 5 most frequent words\n",
    "    print(tweet.text)\n",
    "    print(count_all.most_common(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Count terms only once, equivalent to Document Frequency\n",
    "terms_single = set(terms_all)\n",
    "# Count hashtags only\n",
    "terms_hash = [term for term in preprocess(tweet['text']) \n",
    "              if term.startswith('#')]\n",
    "# Count terms only (no hashtags, no mentions)\n",
    "terms_only = [term for term in preprocess(tweet['text']) \n",
    "              if term not in stop and\n",
    "              not term.startswith(('#', '@'))] \n",
    "              # mind the ((double brackets))\n",
    "              # startswith() takes a tuple (not a list) if \n",
    "              # we pass a list of inputs"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
