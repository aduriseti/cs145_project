{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Data collection\n",
    "\n",
    "### Load packages/set globals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tweepy\n",
    "consumer_key = \"51MI8RrYmzO4btCKG4Qb5uqAa\"\n",
    "consumer_secret = \"ajpPv3Ag0NvMEQLBIwiPyDyU78BbLZn8IS1gTba4x9ZOHNPMNM\"\n",
    "access_token = \"3004471069-VDbNpT9NO0QOtiqKZXkoH5Flv4MArCflIYImXjn\"\n",
    "access_token_secret = \"sP6KMjPZXxYAnaae8bOiauLjCVnx8bzWkBk4KU1iZBxdl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Creating the authentication object\n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "# Setting your access token and secret\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "# Creating the API object while passing in auth information\n",
    "api = tweepy.API(auth) \n",
    "\n",
    "n = 3 # Tweet count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Basic API functionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"A lot of excitement for me.\" @DuaneBrown76 shared how coming to Seattle was the perfect scenario. \n",
      "\n",
      "üì∞ |‚Ä¶ https://t.co/kFng4RmnFE\n",
      "When it comes to critical-thinking skills, Washington students need help, new analysis suggests (By @pgcornwell) https://t.co/rD5whNm77C\n",
      "This just in: The Colts announced Andrew Luck will be placed on Injured Reserve for the rest of the 2017 season. https://t.co/NiQUfF665i\n"
     ]
    }
   ],
   "source": [
    "# Print tweets from my home timeline\n",
    "for status in tweepy.Cursor(api.home_timeline).items(n):\n",
    "    # Process a single status\n",
    "    print(status.text)\n",
    "#     print(status._json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Janet Yellen will be the first person in nearly 40 years to serve no more than one 4-year term as head of the Fed https://t.co/DDnx16d1iZ\n",
      "Now that her children are grown, Michelle Pfeiffer has returned in a head-snapping way this year‚Ä¶ https://t.co/rncgVri57q\n",
      "What did Bernie Sanders learn about Canada's health system? Doctors like free health care as much as patients do. https://t.co/9MRsphQjcu\n"
     ]
    }
   ],
   "source": [
    "# Getting tweets from the New York Times user\n",
    "for tweet in api.user_timeline(id=\"nytimes\", count=n):\n",
    "    # printing the text stored inside the tweet object\n",
    "    print(tweet.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "byr_ne Tweeted: RT @TelegramJames: Sometimes it's fun to see who else used the same stock photo. @NLEmployer's \"Another Way Forward\" and ... https://t.co/K‚Ä¶\n",
      "cleepohl Tweeted: RT @RepAdamSchiff: What kind of president calls our justice system a joke and laughing stock, with ‚Äúso-called‚Äù judges? The worst President‚Ä¶\n",
      "jo_kasprzak Tweeted: RT @MiaFarrow: Reminder: Attorney General Jeff Sessions is the head of our so-called 'laughing stock' justice system\n"
     ]
    }
   ],
   "source": [
    "# get tweets by keyword\n",
    "for tweet in api.search(q=\"stock\", lang=\"en\")[:n]:\n",
    "    # printing the text stored inside the tweet object\n",
    "    print(tweet.user.screen_name,\"Tweeted:\",tweet.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Tweet processing\n",
    "\n",
    "## Tokenization\n",
    "\n",
    "### Default Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['RT', '@', 'TelegramJames', ':', 'Sometimes', 'it', \"'s\", 'fun', 'to', 'see', 'who', 'else', 'used', 'the', 'same', 'stock', 'photo', '.', '@', 'NLEmployer', \"'s\", '``', 'Another', 'Way', 'Forward', \"''\", 'and', '...', 'https', ':', '//t.co/K‚Ä¶']\n"
     ]
    }
   ],
   "source": [
    "# note a default tokenizer doesn't do a good job\n",
    "tweets = api.search(q=\"stock\", lang=\"en\")\n",
    "tweet  = tweets[0].text\n",
    "print(word_tokenize(tweet))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tweet specific tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    " \n",
    "emoticons_str = r\"\"\"\n",
    "    (?:\n",
    "        [:=;] # Eyes\n",
    "        [oO\\-]? # Nose (optional)\n",
    "        [D\\)\\]\\(\\]/\\\\OpP] # Mouth\n",
    "    )\"\"\"\n",
    " \n",
    "regex_str = [\n",
    "    emoticons_str,\n",
    "    r'<[^>]+>', # HTML tags\n",
    "    r'(?:@[\\w_]+)', # @-mentions\n",
    "    r\"(?:\\#+[\\w_]+[\\w\\'_\\-]*[\\w_]+)\", # hash-tags\n",
    "    r'http[s]?://(?:[a-z]|[0-9]|[$-_@.&amp;+]|[!*\\(\\),]|(?:%[0-9a-f][0-9a-f]))+', # URLs\n",
    " \n",
    "    r'(?:(?:\\d+,?)+(?:\\.?\\d+)?)', # numbers\n",
    "    r\"(?:[a-z][a-z'\\-_]+[a-z])\", # words with - and '\n",
    "    r'(?:[\\w_]+)', # other words\n",
    "    r'(?:\\S)' # anything else\n",
    "]\n",
    "    \n",
    "tokens_re = re.compile(r'('+'|'.join(regex_str)+')', re.VERBOSE | re.IGNORECASE)\n",
    "emoticon_re = re.compile(r'^'+emoticons_str+'$', re.VERBOSE | re.IGNORECASE)\n",
    " \n",
    "def chunk(s):\n",
    "    return tokens_re.findall(s)\n",
    " \n",
    "def tokenize(s, lowercase=False):\n",
    "    tokens = tokenize(s)\n",
    "    if lowercase:\n",
    "        tokens = [token if emoticon_re.search(token) else token.lower() for token in tokens]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['RT', '@MiaFarrow', ':', 'Reminder', ':', 'Attorney', 'General', 'Jeff', 'Sessions', 'is', 'the', 'head', 'of', 'our', 'so-called', \"'\", 'laughing', 'stock', \"'\", 'justice', 'system']\n"
     ]
    }
   ],
   "source": [
    "print(tokenize(tweet.text))\n",
    "# ['RT', '@marcobonzanini', ':', 'just', 'an', 'example', '!', ':D', 'http://example.com', '#NLP']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stop terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import string\n",
    " \n",
    "punctuation = list(string.punctuation)\n",
    "stop = stopwords.words('english') + punctuation + ['rt', 'via']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def filt(terms, stop):\n",
    "    return [term for term in terms if term not in stop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RT @TelegramJames: Sometimes it's fun to see who else used the same stock photo. @NLEmployer's \"Another Way Forward\" and ... https://t.co/K‚Ä¶\n",
      "[('photo', 1), ('‚Ä¶', 1), ('else', 1), ('Another', 1), ('Sometimes', 1)]\n",
      "RT @RepAdamSchiff: What kind of president calls our justice system a joke and laughing stock, with ‚Äúso-called‚Äù judges? The worst President‚Ä¶\n",
      "[('‚Ä¶', 2), ('RT', 2), ('stock', 2), ('photo', 1), ('else', 1)]\n",
      "RT @MiaFarrow: Reminder: Attorney General Jeff Sessions is the head of our so-called 'laughing stock' justice system\n",
      "[('RT', 3), ('stock', 3), ('‚Ä¶', 2), ('justice', 2), ('system', 2)]\n"
     ]
    }
   ],
   "source": [
    "import operator \n",
    "import json\n",
    "from collections import Counter\n",
    " \n",
    "count_all = Counter()\n",
    "for tweet in tweets[:3]:\n",
    "    # Create a list with all the terms\n",
    "    terms_all = filt(tokenize(tweet.text), stop)\n",
    "    # Update the counter\n",
    "    count_all.update(terms_all)\n",
    "    # Print the first 5 most frequent words\n",
    "    print(tweet.text)\n",
    "    print(count_all.most_common(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RT @TelegramJames: Sometimes it's fun to see who else used the same stock photo. @NLEmployer's \"Another Way Forward\" and ... https://t.co/K‚Ä¶\n",
      "[('.', 4), ('\"', 2), ('photo', 1), ('‚Ä¶', 1), ('else', 1)]\n",
      "RT @RepAdamSchiff: What kind of president calls our justice system a joke and laughing stock, with ‚Äúso-called‚Äù judges? The worst President‚Ä¶\n",
      "[('.', 4), ('‚Ä¶', 2), ('RT', 2), ('\"', 2), ('stock', 2)]\n",
      "RT @MiaFarrow: Reminder: Attorney General Jeff Sessions is the head of our so-called 'laughing stock' justice system\n",
      "[('.', 4), (':', 4), ('RT', 3), (\"'\", 3), ('stock', 3)]\n"
     ]
    }
   ],
   "source": [
    "import operator \n",
    "import json\n",
    "from collections import Counter\n",
    " \n",
    "count_all = Counter()\n",
    "for tweet in tweets[:3]:\n",
    "    # Create a list with all the terms\n",
    "    terms_all = tokenize(tweet.text)\n",
    "    # Update the counter\n",
    "    count_all.update(terms_all)\n",
    "    # Print the first 5 most frequent words\n",
    "    print(tweet.text)\n",
    "    print(count_all.most_common(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # Count terms only once, equivalent to Document Frequency\n",
    "# terms_single = set(terms_all)\n",
    "# # Count hashtags only\n",
    "# terms_hash = [term for term in preprocess(tweet['text']) \n",
    "#               if term.startswith('#')]\n",
    "# # Count terms only (no hashtags, no mentions)\n",
    "# terms_only = [term for term in preprocess(tweet['text']) \n",
    "#               if term not in stop and\n",
    "#               not term.startswith(('#', '@'))] \n",
    "#               # mind the ((double brackets))\n",
    "#               # startswith() takes a tuple (not a list) if \n",
    "#               # we pass a list of inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk import bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "terms_bigram = bigrams(stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('i', 'me'),\n",
       " ('me', 'my'),\n",
       " ('my', 'myself'),\n",
       " ('myself', 'we'),\n",
       " ('we', 'our'),\n",
       " ('our', 'ours'),\n",
       " ('ours', 'ourselves'),\n",
       " ('ourselves', 'you'),\n",
       " ('you', 'your'),\n",
       " ('your', 'yours'),\n",
       " ('yours', 'yourself'),\n",
       " ('yourself', 'yourselves'),\n",
       " ('yourselves', 'he'),\n",
       " ('he', 'him'),\n",
       " ('him', 'his'),\n",
       " ('his', 'himself'),\n",
       " ('himself', 'she'),\n",
       " ('she', 'her'),\n",
       " ('her', 'hers'),\n",
       " ('hers', 'herself'),\n",
       " ('herself', 'it'),\n",
       " ('it', 'its'),\n",
       " ('its', 'itself'),\n",
       " ('itself', 'they'),\n",
       " ('they', 'them'),\n",
       " ('them', 'their'),\n",
       " ('their', 'theirs'),\n",
       " ('theirs', 'themselves'),\n",
       " ('themselves', 'what'),\n",
       " ('what', 'which'),\n",
       " ('which', 'who'),\n",
       " ('who', 'whom'),\n",
       " ('whom', 'this'),\n",
       " ('this', 'that'),\n",
       " ('that', 'these'),\n",
       " ('these', 'those'),\n",
       " ('those', 'am'),\n",
       " ('am', 'is'),\n",
       " ('is', 'are'),\n",
       " ('are', 'was'),\n",
       " ('was', 'were'),\n",
       " ('were', 'be'),\n",
       " ('be', 'been'),\n",
       " ('been', 'being'),\n",
       " ('being', 'have'),\n",
       " ('have', 'has'),\n",
       " ('has', 'had'),\n",
       " ('had', 'having'),\n",
       " ('having', 'do'),\n",
       " ('do', 'does'),\n",
       " ('does', 'did'),\n",
       " ('did', 'doing'),\n",
       " ('doing', 'a'),\n",
       " ('a', 'an'),\n",
       " ('an', 'the'),\n",
       " ('the', 'and'),\n",
       " ('and', 'but'),\n",
       " ('but', 'if'),\n",
       " ('if', 'or'),\n",
       " ('or', 'because'),\n",
       " ('because', 'as'),\n",
       " ('as', 'until'),\n",
       " ('until', 'while'),\n",
       " ('while', 'of'),\n",
       " ('of', 'at'),\n",
       " ('at', 'by'),\n",
       " ('by', 'for'),\n",
       " ('for', 'with'),\n",
       " ('with', 'about'),\n",
       " ('about', 'against'),\n",
       " ('against', 'between'),\n",
       " ('between', 'into'),\n",
       " ('into', 'through'),\n",
       " ('through', 'during'),\n",
       " ('during', 'before'),\n",
       " ('before', 'after'),\n",
       " ('after', 'above'),\n",
       " ('above', 'below'),\n",
       " ('below', 'to'),\n",
       " ('to', 'from'),\n",
       " ('from', 'up'),\n",
       " ('up', 'down'),\n",
       " ('down', 'in'),\n",
       " ('in', 'out'),\n",
       " ('out', 'on'),\n",
       " ('on', 'off'),\n",
       " ('off', 'over'),\n",
       " ('over', 'under'),\n",
       " ('under', 'again'),\n",
       " ('again', 'further'),\n",
       " ('further', 'then'),\n",
       " ('then', 'once'),\n",
       " ('once', 'here'),\n",
       " ('here', 'there'),\n",
       " ('there', 'when'),\n",
       " ('when', 'where'),\n",
       " ('where', 'why'),\n",
       " ('why', 'how'),\n",
       " ('how', 'all'),\n",
       " ('all', 'any'),\n",
       " ('any', 'both'),\n",
       " ('both', 'each'),\n",
       " ('each', 'few'),\n",
       " ('few', 'more'),\n",
       " ('more', 'most'),\n",
       " ('most', 'other'),\n",
       " ('other', 'some'),\n",
       " ('some', 'such'),\n",
       " ('such', 'no'),\n",
       " ('no', 'nor'),\n",
       " ('nor', 'not'),\n",
       " ('not', 'only'),\n",
       " ('only', 'own'),\n",
       " ('own', 'same'),\n",
       " ('same', 'so'),\n",
       " ('so', 'than'),\n",
       " ('than', 'too'),\n",
       " ('too', 'very'),\n",
       " ('very', 's'),\n",
       " ('s', 't'),\n",
       " ('t', 'can'),\n",
       " ('can', 'will'),\n",
       " ('will', 'just'),\n",
       " ('just', 'don'),\n",
       " ('don', 'should'),\n",
       " ('should', 'now'),\n",
       " ('now', 'd'),\n",
       " ('d', 'll'),\n",
       " ('ll', 'm'),\n",
       " ('m', 'o'),\n",
       " ('o', 're'),\n",
       " ('re', 've'),\n",
       " ('ve', 'y'),\n",
       " ('y', 'ain'),\n",
       " ('ain', 'aren'),\n",
       " ('aren', 'couldn'),\n",
       " ('couldn', 'didn'),\n",
       " ('didn', 'doesn'),\n",
       " ('doesn', 'hadn'),\n",
       " ('hadn', 'hasn'),\n",
       " ('hasn', 'haven'),\n",
       " ('haven', 'isn'),\n",
       " ('isn', 'ma'),\n",
       " ('ma', 'mightn'),\n",
       " ('mightn', 'mustn'),\n",
       " ('mustn', 'needn'),\n",
       " ('needn', 'shan'),\n",
       " ('shan', 'shouldn'),\n",
       " ('shouldn', 'wasn'),\n",
       " ('wasn', 'weren'),\n",
       " ('weren', 'won'),\n",
       " ('won', 'wouldn'),\n",
       " ('wouldn', '!'),\n",
       " ('!', '\"'),\n",
       " ('\"', '#'),\n",
       " ('#', '$'),\n",
       " ('$', '%'),\n",
       " ('%', '&'),\n",
       " ('&', \"'\"),\n",
       " (\"'\", '('),\n",
       " ('(', ')'),\n",
       " (')', '*'),\n",
       " ('*', '+'),\n",
       " ('+', ','),\n",
       " (',', '-'),\n",
       " ('-', '.'),\n",
       " ('.', '/'),\n",
       " ('/', ':'),\n",
       " (':', ';'),\n",
       " (';', '<'),\n",
       " ('<', '='),\n",
       " ('=', '>'),\n",
       " ('>', '?'),\n",
       " ('?', '@'),\n",
       " ('@', '['),\n",
       " ('[', '\\\\'),\n",
       " ('\\\\', ']'),\n",
       " (']', '^'),\n",
       " ('^', '_'),\n",
       " ('_', '`'),\n",
       " ('`', '{'),\n",
       " ('{', '|'),\n",
       " ('|', '}'),\n",
       " ('}', '~'),\n",
       " ('~', 'rt'),\n",
       " ('rt', 'via')]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(terms_bigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
